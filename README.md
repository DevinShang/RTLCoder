# RTL-Coder: Outperforming GPT-3.5 in RTL Code Generation with Our Fully Open-Source Dataset and Lightweight Solution
Shang Liu, Wenji Fang, Yao Lu, Qijun Zhang, Hongce Zhang, and Zhiyao Xie, "RTL-Coder: Outperforming GPT-3.5 in RTL Code Generation with Our Fully Open-Source Dataset and Lightweight Solution"[[paper]](https://arxiv.org/abs/2312.08617)

_**Paper Note**: In the current arxiv version[[paper]](https://arxiv.org/abs/2312.08617), the Figure 12(a) should be the results of functionality of EvalMachine and Figure 12(b) should be the results of syntax._

_**Repo Note**: The training scripts and data generation flow are coming soon. We are also still actively further improving and validating RTLCoder. This is version V1.0. If you are interested, please kindly monitor our latest update on Arxiv and Github repo in the near future._

[RTLCoder-Z-v1.0](https://huggingface.co/ishorn5/RTLCoder-Z-v1.0) is based on Zephyr-7b-beta and finetuned on our collected dataset. The 4bit version [RTLCoder-Z-GPTQ4bit-v1.0](https://huggingface.co/ishorn5/RTLCoder-Z-GPTQ4bit-v1.0) with 4GB was obtained by GPTQ conversion. 

